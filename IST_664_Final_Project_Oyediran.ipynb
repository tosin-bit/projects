{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlZbTR4pgpyb"
      },
      "source": [
        "  import os\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import nltk\n",
        "from nltk import FreqDist, word_tokenize, bigrams\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder, TrigramAssocMeasures, TrigramCollocationFinder\n",
        "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
        "from nltk.text import Text \n",
        "from sklearn.model_selection import train_test_split\n",
        "#from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set(style = 'white')\n",
        "sns.set(style = 'whitegrid', color_codes = True)\n",
        "import random\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3qkvy8HgS0W"
      },
      "source": [
        "spamDir = 'corpus/spam' # directory of SPAM emails\n",
        "hamDir = 'corpus/ham' # directory of HAM emails\n",
        "\n",
        "# function to get absolute filepaths in a directory\n",
        "# param directory: absolute directory name\n",
        "# yields absolute file paths\n",
        "def absoluteFilePaths(directory):\n",
        "   for dirpath,_,filenames in os.walk(directory):\n",
        "       for f in filenames:\n",
        "           yield os.path.abspath(os.path.join(dirpath, f))\n",
        "\n",
        "# function to do baseline processing of emails from a directory into a list of tokenized documents\n",
        "# param directory: absolute directory name\n",
        "# param label: either 'spam' or 'ham'\n",
        "# returns texts and documents\n",
        "def process(directory, label):    \n",
        "    texts = [] # initialize list of strings where each string is an email\n",
        "    # get list of absolute file paths in directory\n",
        "    myGenerator = absoluteFilePaths(directory)\n",
        "    filelist = []\n",
        "    for f in myGenerator:\n",
        "        filelist.append(f)\n",
        "    # process all files in directory that end in .txt\n",
        "    for f in filelist:\n",
        "        if (f.endswith(\".txt\")):\n",
        "            # open file for reading and read entire file into a string            \n",
        "            with open(f, 'r', encoding = 'latin-1') as fin:\n",
        "                texts.append(fin.read())\n",
        "    documents = [] # initialize list of tuples where each element is a tokenized email with its label\n",
        "    # process each email\n",
        "    for text in texts:\n",
        "        tokens = word_tokenize(text)\n",
        "        documents.append((tokens, label))\n",
        "    return texts, documents\n",
        "\n",
        "\n",
        "# function to compute basic corpus statistics for either spam or ham\n",
        "# param texts: a list of strings where each element is email text\n",
        "# param documents: a list of tuples where the first item of each tuple is the tokenized email text\n",
        "# prints corpus statistics\n",
        "def getstats(texts, documents):\n",
        "    text_list = [text for text in texts]\n",
        "    doc_list = [doc[0] for doc in documents]\n",
        "    print(\"Email level statistics:\\n\")\n",
        "    # average number of characters per email\n",
        "    avg_chars = int(sum([len(t) for t in text_list]) / len(text_list))\n",
        "    print(\"Average number of characters per email: {:d}\".format(avg_chars))\n",
        "    # average number of words per email\n",
        "    avg_words = int(sum([len(doc) for doc in doc_list]) / len(doc_list))\n",
        "    print(\"Average number of words per email: {:d}\".format(avg_words))\n",
        "    # average vocabulary size per email\n",
        "    avg_vocab = int(sum([len(set(doc)) for doc in doc_list]) / len(doc_list))\n",
        "    print(\"Average vocabulary size per email: {:d}\".format(avg_vocab))\n",
        "    # average lexical richness per email (proportion of unique words to total words)\n",
        "    avg_lex_rich = sum([len(set(doc))/len(doc) for doc in doc_list]) / len(doc_list)\n",
        "    print(\"Average lexical richness per email: {:.2f}\".format(avg_lex_rich))\n",
        "    print(\"\\nCorpus level statistics:\\n\")\n",
        "    words = []\n",
        "    for doc in doc_list:\n",
        "        words.extend(doc)\n",
        "    # total number of words\n",
        "    print(\"Total number of words: {:d}\".format(len(words)))\n",
        "    # vocabulary size\n",
        "    print(\"Total vocabulary size: {:d}\".format(len(set(words))))\n",
        "    # lexical richness\n",
        "    print(\"Total lexical richness: {:.2f}\".format(len(set(words)) / len(words)))\n",
        "    # average number of characters per word\n",
        "    word_lengths = [len(w) for w in words]\n",
        "    print(\"Average number of characters per word: {:.2f}\".format(sum(word_lengths) / len(word_lengths)))    \n",
        "\n",
        "# function to extract tokens from documents\n",
        "# param documents: a list of tuples where the first item of each tuple is the tokenized email text\n",
        "# returns a single list of all tokens from email documents\n",
        "def getTokens(documents):\n",
        "    tokens = []\n",
        "    for doc in documents:\n",
        "        for w in doc[0]:\n",
        "            tokens.append(w)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# function to print top n bigram frequency distribution\n",
        "# param documents: a list of tuples where the first item of each tuple is the tokenized email text\n",
        "# param type: 'freq' (frequency) or 'mi' (mutual information)\n",
        "# param n: top n parameter for bigrams to print\n",
        "# prints results\n",
        "# returns scored bigrams\n",
        "def getBigramDist(documents, n, type = 'freq'):\n",
        "    tokens = [w.lower() for w in getTokens(documents)]\n",
        "    measures = BigramAssocMeasures()\n",
        "    finder = BigramCollocationFinder.from_words(tokens) # scorer\n",
        "    finder.apply_word_filter(alpha_filter) # exclude non-alphabetic words\n",
        "    finder.apply_word_filter(lambda w: w in stopwords) # exclude stop words\n",
        "    if type == 'mi':\n",
        "        finder.apply_freq_filter(5) # frequency filter of greater than or equal to 5\n",
        "        scored = finder.score_ngrams(measures.pmi) # distribution of mutual information\n",
        "        print(\"Top 100 most common strongly connected bigrams out of total {:d} unique bigrams:\\n\".format(len(scored)))\n",
        "    else:\n",
        "        # distribution of frequency as proportion of the bigram count to count of all bigrams\n",
        "        scored = finder.score_ngrams(measures.raw_freq)\n",
        "        print(\"Top 100 most common bigrams out of total {:d} unique bigrams:\\n\".format(len(scored)))\n",
        "    print([score[0] for score in scored[:n]])\n",
        "    return scored\n",
        "\n",
        "# define stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stopwords1 = stopwords.words('english')\n",
        "\n",
        "# function that identifies non-alphabetic tokens\n",
        "# param w: string word\n",
        "# returns true if word consists only of non-alphabetic characters \n",
        "def alpha_filter(w):\n",
        "    # pattern to match a word of non-alphabetical characters\n",
        "    pattern = re.compile('^[^a-z]+$')\n",
        "    if pattern.match(w):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# function to clean a list of tokens [basic]\n",
        "# param tokens: a list of strings where each element is a token\n",
        "# returns a new list of cleaned tokens\n",
        "def clean1(tokens):\n",
        "    # convert tokens to lower-case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove non-alphabetic words\n",
        "    tokens = [w for w in tokens if not alpha_filter(w)]    \n",
        "    # remove stop words\n",
        "    tokens = [w for w in tokens if not w in stopwords1]\n",
        "    return tokens\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkCAmeD2hJFd"
      },
      "source": [
        "\n",
        "%%time\n",
        "spamTexts, spamDocs = process(spamDir, 'spam')\n",
        "hamTexts, hamDocs = process(hamDir, 'ham')\n",
        "print(\"Total number of SPAM documents read: {:d}\".format(len(spamDocs)))\n",
        "print(\"Total number of HAM documents read: {:d}\".format(len(hamDocs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RAoklNDhPtI"
      },
      "source": [
        "## **Exploration**\n",
        "- Corpus Statistics\n",
        "- Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA0WglYMhwFi"
      },
      "source": [
        "## **Corpus Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJzoyt03hnez"
      },
      "source": [
        "getstats(spamTexts, spamDocs)\n",
        "getstats(hamTexts, hamDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzxoKDx8iE1z"
      },
      "source": [
        "Observations:\n",
        "\n",
        "On a per email basis, SPAM emails are generally longer than HAM emails and contain more unique words.\n",
        "SPAM emails measure higher in lexical richness than HAM emails\n",
        "Actions:\n",
        "\n",
        "Add features measuring document statistics for modeling as these seem to distinguish SPAM and HAM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXKJ8s2WiI5Q"
      },
      "source": [
        "# inspect first 50 tokens\n",
        "print(getTokens(spamDocs)[:50])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0aV_EcTiOB1"
      },
      "source": [
        "\n",
        "# inspect first 50 tokens\n",
        "print(getTokens(hamDocs)[:50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VT0W0VTiR_z"
      },
      "source": [
        "Upon examination, it is clear that both categories could mutually benefit from applying some basic cleaning transformations.\n",
        "\n",
        "Observations:\n",
        "\n",
        "There are non-alphabetic words in the basic tokenization\n",
        "Tokens are case-sensitive, which for the purposes of SPAM detection may not be a necessity\n",
        "The word 'Subject' is commonplace and may not be useful for distinguishing SPAM/HAM\n",
        "Actions:\n",
        "\n",
        "Remove non-alphabetic words\n",
        "Convert tokens to lower-case\n",
        "Define and add to Stopwords list words that are highly commonplace such as: 'the'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOHz4PLeiS-O"
      },
      "source": [
        "\n",
        "# do an basic cleaning of the original spam and ham tokens\n",
        "spamTokens = clean1(getTokens(spamDocs))\n",
        "hamTokens = clean1(getTokens(hamDocs))\n",
        "\n",
        "print(\"There are {:d} SPAM tokens\".format(len(spamTokens)))\n",
        "print(\"There are {:d} HAM tokens\".format(len(hamTokens)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbivPOuaiaRz"
      },
      "source": [
        "\n",
        "## **Top 50 words by frequency - SPAM¶**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh-PAcR2ic9Z"
      },
      "source": [
        "fdistspam = FreqDist(spamTokens) # frequency distribution\n",
        "print([item for item in fdistspam.most_common(50)])\n",
        "fdistspam.plot(50, cumulative = True, title = \"SPAM: Cumulative Frequency of Top 50 words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7YiDuHqisUK"
      },
      "source": [
        "## **Top 50 words by frequency - HAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie9vo0WdisAx"
      },
      "source": [
        "fdistham = FreqDist(hamTokens) # frequency distribution\n",
        "print([item for item in fdistham.most_common(50)])\n",
        "fdistham.plot(50, cumulative = True, title = \"HAM: Cumulative Frequency of Top 50 words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZTqetoni54G"
      },
      "source": [
        "Upon examination of the unigram frequency distributions for both SPAM and HAM, it is clear that the top 50 words for HAM make up a greater proportion of its total word frequency relative to SPAM. This was expected as SPAM exhibited to have a more extensive vocabulary in previous corpus statistics.\n",
        "\n",
        "Observations: Distinguishing features observed for SPAM includes words like ('http', 'www'). The top 50 frequency lexicon for SPAM appears quite distinct from HAM. Distinguishing features observed for HAM includes words like 'ect' (Enron Capital and Trade) and other corporate references.\n",
        "\n",
        "Actions:\n",
        "\n",
        "Identify more high frequency words that are common to both SPAM and HAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHMOAr3Mi7Pq"
      },
      "source": [
        "#Measure the Overlap of High Frequency Words Between SPAM and HAM\n",
        "print(\"Top N Proportion of Word Overlap:\\n\")\n",
        "top_n_list = [50, 100, 500, 1000, 2000, 3000]\n",
        "for n in top_n_list:\n",
        "    print(n, len(set([t[0] for t in fdistham.most_common(n)]) & set([t[0] for t in fdistspam.most_common(n)])) / n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCv1NbqEjQU1"
      },
      "source": [
        " **Sample of Common Words between SPAM and HAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIazYO0jjN-k"
      },
      "source": [
        "n = 100\n",
        "high_freq_common = list(set([t[0] for t in fdistham.most_common(n)]) & set([t[0] for t in fdistspam.most_common(n)]))\n",
        "print(\"There are {:d} highly frequent words that are common to both SPAM and HAM:\\n\".format(len(high_freq_common)))\n",
        "print(high_freq_common)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMJJ0KzBjmFA"
      },
      "source": [
        "#Sample of Least Common Words in SPAM and HAM\n",
        "print(\"Sample of the least common words in SPAM:\\n\")\n",
        "print(fdistspam.hapaxes()[:50])\n",
        "print(\"\\nThere are {:d} hapaxes in the SPAM text\".format(len(fdistspam.hapaxes()))\n",
        "\n",
        "print(\"Sample of the least common words in HAM:\\n\")\n",
        "print(fdistham.hapaxes()[:50])\n",
        "print(\"\\nThere are {:d} hapaxes in the HAM text\".format(len(fdistham.hapaxes())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uydSFrzQjzTH"
      },
      "source": [
        "Observations:\n",
        "\n",
        "The top 50 - 100 words for SPAM and HAM seem to be mutually exclusive for the most part. However, just the top 50-100 words may be insufficient in classifying SPAM emails due to its large vocabulary, and unigram features for classification will likely need to be substantially extended\n",
        "There are 26 highly frequent words that are common to both SPAM and HAM. For the purposes of this analysis, it might be beneficial to remove these words by adding them to the Stopwords list.\n",
        "SPAM emails contain almost four times as many hapaxes (very uncommon words) compared to HAM emails. For the purposes of this analysis, it might be beneficial to remove the hapaxes by adding them to the Stopwords list.\n",
        "Actions:\n",
        "\n",
        "Define and add to Stopwords list: 'message', 'x', 'please', 'mail', 'could', 'like', 'us', 'gas', 'price', 'may', 'time', 'get', 'see', 'net', 'need', 'would', 'l', 'information', 'th', 'company', 'p', 'new', 'e', 'one', 'also', 'com'\n",
        "These are high frequency words that are common to both SPAM and HAM; therefore these words will be added to Stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcCFd8Fej0S6"
      },
      "source": [
        "# extensive stopwords - includes high frequency words common to both labels and hapaxes (highly infrequent words)\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "addtlstopwords = ['subject'] + high_freq_common + fdistspam.hapaxes() + fdistham.hapaxes()\n",
        "stopwords2 = stopwords + addtlstopwords\n",
        "print(\"There are a total of {:d} stopwords defined\".format(len(stopwords2)))\n",
        "\n",
        "# function to clean a list of tokens [advanced]\n",
        "# param tokens: a list of strings where each element is a token\n",
        "# returns a new list of cleaned tokens\n",
        "def clean2(tokens):\n",
        "    # convert tokens to lower-case\n",
        "    tokens = [w.lower() for w in tokens]\n",
        "    # remove non-alphabetic words\n",
        "    tokens = [w for w in tokens if not alpha_filter(w)]    \n",
        "    # remove stop words\n",
        "    tokens = [w for w in tokens if not w in stopwords2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuxRO4AJj67Z"
      },
      "source": [
        "# **Top 100 Bigrams by Frequency - SPAM and HAM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIPzGdhnkFfk"
      },
      "source": [
        "bigramSpamFreq = getBigramDist(spamDocs, n = 100, type = 'freq')\n",
        "\n",
        "bigramHamFreq = getBigramDist(hamDocs, n = 100, type = 'freq')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUXt9YkDkRc2"
      },
      "source": [
        "## ***Top 100 Bigrams by Mutual Information ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO3GBdJKkfD-"
      },
      "source": [
        "bigramSpamMI = getBigramDist(spamDocs, n = 100, type = 'mi')\n",
        "bigramHamMI = getBigramDist(hamDocs, n = 100, type = 'mi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qylfLA9mkxye"
      },
      "source": [
        "**Typical Frequency Scores for SPAM and HAM bigrams**\n",
        "\n",
        "\n",
        "Distributional statistics computed using mean and median bigram scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4B10g7Sk0vw"
      },
      "source": [
        "mean_score_freq_spam = np.mean([s[1] for s in bigramSpamFreq])\n",
        "median_score_freq_spam = np.median([s[1] for s in bigramSpamFreq])\n",
        "print(\"SPAM frequency scores:\\n\")\n",
        "print(\"Mean: {:.2E} Median: {:.2E}\".format(mean_score_freq_spam, median_score_freq_spam))\n",
        "mean_score_freq_ham = np.mean([s[1] for s in bigramHamFreq])\n",
        "median_score_freq_ham = np.median([s[1] for s in bigramHamFreq])\n",
        "print(\"\\nHAM frequency scores:\\n\")\n",
        "print(\"Mean: {:.2E} Median: {:.2E}\".format(mean_score_freq_ham, median_score_freq_ham))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEI4lFvuk-l4"
      },
      "source": [
        "SPAM frequency scores:\n",
        "\n",
        "Mean: 4.13E-06 Median: 2.81E-06\n",
        "\n",
        "HAM frequency scores:\n",
        "\n",
        "Mean: 3.47E-06 Median: 1.20E-06\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xnzGt4Lk_lg"
      },
      "source": [
        "a = [s[1] for s in bigramSpamFreq]\n",
        "plt.hist(a)\n",
        "plt.title(\"Histogram of SPAM Bigram Frequency Scores\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "a = [s[1] for s in bigramHamFreq]\n",
        "plt.hist(a)\n",
        "plt.title(\"Histogram of HAM Bigram Frequency Scores\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URL9695klIBv"
      },
      "source": [
        "**Typical Mutual Information Scores for SPAM and HAM bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IU5BkpzlLIr"
      },
      "source": [
        "\n",
        "mean_score_mi_spam = np.mean([s[1] for s in bigramSpamMI])\n",
        "median_score_mi_spam = np.median([s[1] for s in bigramSpamMI])\n",
        "print(\"SPAM MI scores:\\n\")\n",
        "print(\"Mean: {:.2E} Median: {:.2E}\".format(mean_score_mi_spam, median_score_mi_spam))\n",
        "mean_score_mi_ham = np.mean([s[1] for s in bigramHamMI])\n",
        "median_score_mi_ham = np.median([s[1] for s in bigramHamMI])\n",
        "print(\"\\nHAM MI scores:\\n\")\n",
        "print(\"Mean: {:.2E} Median: {:.2E}\".format(mean_score_mi_ham, median_score_mi_ham))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhTVUU2llRol"
      },
      "source": [
        "SPAM MI scores:\n",
        "\n",
        "Mean: 1.04E+01 Median: 1.03E+01\n",
        "\n",
        "HAM MI scores:\n",
        "\n",
        "Mean: 8.56E+00 Median: 8.46E+00"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd57EmXnlTqc"
      },
      "source": [
        "a = [s[1] for s in bigramSpamMI]\n",
        "plt.hist(a)\n",
        "plt.title(\"Histogram of SPAM Bigram Mutual Information Scores\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "a = [s[1] for s in bigramHamMI]\n",
        "plt.hist(a)\n",
        "plt.title(\"Histogram of HAM Bigram Mutual Information Scores\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdGXOqP-lYlq"
      },
      "source": [
        "Observations:\n",
        "\n",
        "Bigrams for SPAM exhibited more references to retail product entities, references to selling, and non-English phrases.\n",
        "Bigrams for HAM exhibited more repeated references to organization, person, and location entities.\n",
        "Bigrams for SPAM were on average more frequent than HAM by a small order of magnitude; the same could be said based on mutual information scores.\n",
        "Frequency scores exhibit a right-skewed distribution, whereas mutual information scores feature a more normal distribution.\n",
        "Actions:\n",
        "\n",
        "Complete lists of bigrams were extracted for modeling and feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMVPaF9MliWI"
      },
      "source": [
        "# **Part-of-Speech Tag Frequencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSWHvMe-lnpK"
      },
      "source": [
        "# function to print normalized frequencies of POS tags\n",
        "# param documents: a list of tuples where the first item of each tuple is the tokenized email text\n",
        "# prints normalized frequencies for nouns, verbs, adjectives, and adverbs\n",
        "def getPosStats(documents):\n",
        "    docs = documents\n",
        "    # get list of tags\n",
        "    pos = [t[1] for t in nltk.pos_tag(docs)]\n",
        "    # aggregate class counts\n",
        "    noun_count, verb_count, adj_count, adv_count = 0, 0, 0, 0\n",
        "    for tag in pos:\n",
        "        if tag.startswith('N'): noun_count += 1\n",
        "        if tag.startswith('V'): verb_count += 1\n",
        "        if tag.startswith('J'): adj_count += 1\n",
        "        if tag.startswith('R'): adv_count += 1\n",
        "    # normalize class counts\n",
        "    noun_count_norm = noun_count / len(pos); print(\"Normalized Noun Frequency: {:.2f}\".format(noun_count_norm))\n",
        "    verb_count_norm = verb_count / len(pos); print(\"Normalized Verb Frequency: {:.2f}\".format(verb_count_norm))\n",
        "    adj_count_norm = adj_count / len(pos); print(\"Normalized Adjective Frequency: {:.2f}\".format(adj_count_norm))\n",
        "    adv_count_norm = adv_count / len(pos); print(\"Normalized Adverb Frequency: {:.2f}\".format(adv_count_norm))\n",
        "\n",
        "getPosStats(spamTokens)\n",
        "getPosStats(hamTokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfIWY7N9mKSy"
      },
      "source": [
        "Observations:\n",
        "\n",
        "At an aggregate level, both SPAM and HAM appear to be similar in their POS frequency distributions.\n",
        "SPAM tokens contain slightly more nouns and adjectives, and fewer verbs.\n",
        "Actions:\n",
        "\n",
        "Include POS tag features in the modeling experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USQ57A_gmMjL"
      },
      "source": [
        "## **Modeling Experiments**\n",
        "*Feature Engineering\n",
        "  - ngrams (bag of words)\n",
        "    - unigrams -\n",
        "    - bigrams\n",
        "  - word statistics (lexical richness/email, # characters/email, # words/email, mean # of characters per word)\n",
        "  - POS tag features\n",
        "*Modeling\n",
        "  - NB NLTK\n",
        "  - Multinomial NB Sci-kit Learn\n",
        "  - SVM Sci-kit Learn\n",
        "  - Logistic Regression Sci-kit Learn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6it3Vpk7mhJx"
      },
      "source": [
        "# combine labeled documents\n",
        "print(\"There are {:d} SPAM documents and {:d} HAM documents\".format(len(spamDocs), len(hamDocs)))\n",
        "documents = spamDocs + hamDocs\n",
        "print(\"There are a total of {:d} documents\".format(len(documents)))\n",
        "\n",
        "\n",
        "\n",
        "#Randomly shuffle the documents for training and testing classifiers\n",
        "random.seed(111)\n",
        "random.shuffle(documents)\n",
        "print([doc[1] for doc in documents[:20]]) # demonstrate labels have been shuffled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L8nwuESmx9R"
      },
      "source": [
        "## **Experiment 1: Testing the Application of Stopwords**\n",
        "The objective of this experiment is to test whether the application of an extensive stopwords list provides for improved classification results relative to a basic stopwords list from NLTK.\n",
        "\n",
        "The extensive stopwords list contains 28056 words and includes:\n",
        "\n",
        "- basic stopwords\n",
        "- high-frequency words that are common to both classes\n",
        "- hapaxes\n",
        "Features: 3000 Unigrams\n",
        "\n",
        "Top 3000 words based on frequency varied by the stopwords applied\n",
        "Classifier: Naive Bayes Classifier from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9UgDPkfnSki"
      },
      "source": [
        "# function to get word features\n",
        "# param documents: a list of tuples where the first item of each tuple is the tokenized email text\n",
        "# param stopwords: a list of strings where each element is a stopword\n",
        "# returns a list of 3000 strings where each string is a word feature\n",
        "def getWordFeatures(documents, stopwords):    \n",
        "    # lower-case conversion of complete document tokenization\n",
        "    all_words_list = [word.lower() for (email, cat) in documents for word in email]\n",
        "    # filter for alphabetic words\n",
        "    all_words_list = [word for word in all_words_list if not alpha_filter(word)]\n",
        "    # exclude stopwords\n",
        "    keep_words = set(all_words_list) - set(stopwords)\n",
        "    all_words_list = [word for word in all_words_list if word in keep_words]\n",
        "    all_words = FreqDist(all_words_list)\n",
        "    # get the 1500 most frequently appearing keywords in all words\n",
        "    word_items = all_words.most_common(3000)\n",
        "    word_features = [word for (word, count) in word_items]\n",
        "    return word_features\n",
        "\n",
        "# [feature definition function: experiment 1] function to get document features (applicable for unigrams)\n",
        "# param document: a list of strings representing a tokenized email\n",
        "# param word_features: a list of strings against which the tokens in document are matched\n",
        "# returns a dictionary where each key is 'contains(keyword)' and is either true or false\n",
        "def document_features(document, word_features):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['V_{}'.format(word)] = (word in document_words)\n",
        "    return features\n",
        "\n",
        "# function to get feature sets for modeling in Experiment 1\n",
        "def getFeatureSets1(documents, stopwords):\n",
        "    # get word features based on specified stopwords\n",
        "    word_features = getWordFeatures(documents, stopwords)\n",
        "    featuresets = [(document_features(d, word_features), c) for (d, c) in documents]\n",
        "    return featuresets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U20IyUBTnWDv"
      },
      "source": [
        "## **Feature Extraction**\n",
        "- Two Unigram feature sets that either use basic or extensive stopwords lists\n",
        "  - unigramBasic3000 uses 3000 word features having excluded basic stopwords (i.e. out-of-box NLTK)\n",
        "  - unigramExt3000 uses 3000 word features having excluded extensive stopwords (i.e. basic stopwords, highly common keywords, hapaxes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYrd4BKpnfYq"
      },
      "source": [
        "# get feature sets for documents using basic stopwords\n",
        "unigramBasic3000 = getFeatureSets1(documents, stopwords1)\n",
        "# split training and test sets 70/30\n",
        "unigramBasic3000Train = unigramBasic3000[:3620]\n",
        "unigramBasic3000Test = unigramBasic3000[3620:]\n",
        "\n",
        "# get feature sets for documents using extensive stopwords\n",
        "unigramExt3000 = getFeatureSets1(documents, stopwords2)\n",
        "# split training and test sets 70/30\n",
        "unigramExt3000Train = unigramExt3000[:3620]\n",
        "unigramExt3000Test = unigramExt3000[3620:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbqM1ddnnkc"
      },
      "source": [
        "## **Naive Bayes Classifier in NLTK**\n",
        "\n",
        "Compute Precision, Recall, F1 and Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rCQJIPZnpNc"
      },
      "source": [
        "# function to compute precision, recall, and f1 for each label and for any number of labels\n",
        "# param gold: list of strings where each element is a gold label\n",
        "# param predicted: list of strings where each element is a predicted label (in same order)\n",
        "# output: prints precision, recall, f1 for each class\n",
        "def eval_measures(gold, predicted):\n",
        "    # get a list of labels\n",
        "    labels = list(set(gold))\n",
        "    # initialize list of class-specific scores\n",
        "    precision_list, recall_list, f1_list = [],[],[]\n",
        "    for lab in labels:\n",
        "        # for each label, compare gold and predicted lists and compute values\n",
        "        TP = FP = FN = TN = 0\n",
        "        for i, val in enumerate(gold):\n",
        "            if val == lab and predicted[i] == lab:  TP += 1\n",
        "            if val == lab and predicted[i] != lab:  FN += 1\n",
        "            if val != lab and predicted[i] == lab:  FP += 1\n",
        "            if val != lab and predicted[i] != lab:  TN += 1\n",
        "        # formulas for precision, recall, and f1\n",
        "        precision = TP / (TP + FN)\n",
        "        recall = TP / (TP + FP)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "        f1_list.append( 2 * (recall * precision) / (recall + precision))\n",
        "    # the evaluation measures in a table with one row per label\n",
        "    print('class\\tPrecision\\tRecall\\tF1\\n')\n",
        "    # print measures for each label\n",
        "    for i, lab in enumerate(labels):\n",
        "        print(lab, '\\t', \"{:10.3f}\".format(precision_list[i]), \\\n",
        "          \"{:10.3f}\".format(recall_list[i]), \"{:10.3f}\".format(f1_list[i]))\n",
        "        \n",
        "# function to perform cross-validation and model evaluation for experiment 1\n",
        "# param num_folds: integer specifying number of iterations\n",
        "# param featuresets: list containing dictionary of features and label where each element is an email\n",
        "# output: prints iteration accuracy and the mean accuracy\n",
        "def cross_validation_accuracy(num_folds, featuresets):\n",
        "    subset_size = int(len(featuresets)/num_folds)\n",
        "    print('Each fold size:', subset_size, '\\n')    \n",
        "    accuracy_list = []\n",
        "    # iterate over the folds\n",
        "    for i in range(num_folds):\n",
        "        test_this_round = featuresets[(i*subset_size):][:subset_size]\n",
        "        train_this_round = featuresets[:(i*subset_size)] + featuresets[((i+1)*subset_size):]\n",
        "        # train using train_this_round\n",
        "        classifier = nltk.NaiveBayesClassifier.train(train_this_round)\n",
        "        # evaluate against test_this_round and save accuracy\n",
        "        accuracy_this_round = nltk.classify.accuracy(classifier, test_this_round)        \n",
        "        print (i, 'accuracy:', accuracy_this_round)\n",
        "        accuracy_list.append(accuracy_this_round)        \n",
        "    # find mean accuracy over all rounds\n",
        "    print ('mean accuracy', sum(accuracy_list) / num_folds)\n",
        "# function to plot confusion matrix\n",
        "# param gold: list of strings where each element is a gold label\n",
        "# param predicted: list of strings where each element is a predicted label (in same order)\n",
        "# output: plots confusion matrix with sklearn\n",
        "def getCM(gold, predicted):\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm = confusion_matrix(gold, predicted)\n",
        "    # plot heatmap\n",
        "    class_names=[0,1] # name  of classes\n",
        "    fig, ax = plt.subplots()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    sns.heatmap(pd.DataFrame(cm), annot=True, cmap = \"YlGnBu\", fmt = 'g')\n",
        "    ax.xaxis.set_label_position(\"top\")\n",
        "    plt.tight_layout()\n",
        "    plt.title('Confusion matrix', y=1.1)\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "# function to produce precision, recall, f1, and confusion matrix\n",
        "# param train: training set\n",
        "# param test: test set\n",
        "# output: prints precision, recall, f1, and plots a confusion matrix\n",
        "def getClassScores(train, test):\n",
        "    # train a classifier\n",
        "    classifier = nltk.NaiveBayesClassifier.train(train)\n",
        "    # get actuals and predictions\n",
        "    goldlist, predictedlist = [],[]\n",
        "    for (features, label) in test:\n",
        "        goldlist.append(label)\n",
        "        predictedlist.append(classifier.classify(features))\n",
        "    # print evaluation measures\n",
        "    eval_measures(goldlist, predictedlist)\n",
        "    # plot confusion matrix\n",
        "    getCM(goldlist, predictedlist)     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3w6XcXYn_53"
      },
      "source": [
        "# **Model Evaluation: Basic and Extensive Stopwords List**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTWFb7EEoCKg"
      },
      "source": [
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, unigramBasic3000Train)\n",
        "# train a classifier and predict test set; get evaluation metrics; plot confusion matrix\n",
        "getClassScores(unigramBasic3000Train, unigramBasic3000Test)\n",
        "\n",
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, unigramExt3000Train)\n",
        "# train a classifier and predict test set; get evaluation metrics; plot confusion matrix\n",
        "getClassScores(unigramExt3000Train, unigramExt3000Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TME-R7cnoTIF"
      },
      "source": [
        "# **Experiment 2: Testing the Effectiveness of Bigrams**\n",
        "\n",
        "The objective of this experiment is to test whether adding bigrams provides for improved classification results relative to just utilizing unigrams.\n",
        "\n",
        "Baseline featureset:\n",
        "\n",
        "- Top 3000 unigrams by frequency with basic stopwords applied, as demonstrated in Experiment 1\n",
        "Test featureset:\n",
        "\n",
        "- Top 3000 unigrams by frequency with basic stopwords exclusion\n",
        "- Top 1000 bigrams scored by frequency with basic stopwords exclusion\n",
        "Classifier: Naive Bayes Classifier from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_eK1iW-oSAc"
      },
      "source": [
        "# function to get bigram features\n",
        "# param documents: a list of tuples where the first item of each tuple is the tokenized email text\n",
        "# param stopwords: a list of strings where each element is a stopword\n",
        "# returns a list of 1000 tuples where each element is a bigram feature\n",
        "def getBigramFeatures(documents, stopwords):\n",
        "    # lower-case conversionof complete document tokenization\n",
        "    all_words_list = [word.lower() for (email, cat) in documents for word in email]\n",
        "    # Top 1000 bigram feature extraction\n",
        "    measures = BigramAssocMeasures()\n",
        "    finder = BigramCollocationFinder.from_words(all_words_list) # scorer\n",
        "    finder.apply_word_filter(alpha_filter) # exclude non-alphabetic words\n",
        "    finder.apply_word_filter(lambda w: w in stopwords) # exclude stop words    \n",
        "    scored = finder.score_ngrams(measures.raw_freq)\n",
        "    bigram_features = [s[0] for s in scored[:1000]]\n",
        "    return bigram_features\n",
        "\n",
        "# [feature definition function: experiment 2] function to get document features (applicable for unigrams and bigrams)\n",
        "# param document: a list of strings representing a tokenized email\n",
        "# param word_features: a list of strings against which the tokens in document are matched\n",
        "# param bigram_features: a list of tuples where each element is a bigram\n",
        "# returns a dictionary where each key is 'contains(keyword)' and is either true or false\n",
        "def bigram_document_features(document, word_features, bigram_features):\n",
        "    document_words = set(document)\n",
        "    document_bigrams = nltk.bigrams(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['V_{}'.format(word)] = (word in document_words)\n",
        "    for bigram in bigram_features:\n",
        "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)\n",
        "    return features\n",
        "\n",
        "# function to get feature sets for modeling in Experiment 2\n",
        "def getFeatureSets2(documents, stopwords):\n",
        "    # get word features based on specified stopwords\n",
        "    word_features = getWordFeatures(documents, stopwords)\n",
        "    # get bigram features based on specified stopwords\n",
        "    bigram_features = getBigramFeatures(documents, stopwords)\n",
        "    featuresets = [(bigram_document_features(d, word_features, bigram_features), c) for (d, c) in documents]\n",
        "    return featuresets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n7GuWEDopMI"
      },
      "source": [
        "Feature Extraction\n",
        "- Two feature sets that either use unigrams only or unigrams + bigrams\n",
        "    - unigramBasic3000 uses 3000 word features having excluded basic stopwords (defined in Experiment 1)\n",
        "    - bigramBasic4000 uses 3000 word features and 1000 bigram features having excluded basic stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDtyX8zeoxYq"
      },
      "source": [
        "# get feature sets for documents using basic stopwords\n",
        "bigramBasic4000 = getFeatureSets2(documents, stopwords1)\n",
        "# split training and test sets 70/30\n",
        "bigramBasic4000Train = bigramBasic4000[:3620]\n",
        "bigramBasic4000Test = bigramBasic4000[3620:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCnqq6w6o4lg"
      },
      "source": [
        "# **Model Evaluation: Unigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evimtLbko2dF"
      },
      "source": [
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, unigramBasic3000Train)\n",
        "# train a classifier and predict test set; get evaluation metrics; plot confusion matrix\n",
        "getClassScores(unigramBasic3000Train, unigramBasic3000Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZBFVQm5o95t"
      },
      "source": [
        "# **Model Evaluation: Unigrams + Bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR28U3OapAGb"
      },
      "source": [
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, bigramBasic4000Train)\n",
        "# train a classifier and predict test set: get evaluation metrics; plot confusion matrix\n",
        "getClassScores(bigramBasic4000Train, bigramBasic4000Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF-9MwzPpVbD"
      },
      "source": [
        "# **Experiment 3: Testing the Effectiveness of Part-of-Speech Tags**\n",
        "The objective of this experiment is to test whether adding POS tags provides for improved classification results relative to utilizing unigrams and bigrams. In earlier exploration of POS tag frequencies for SPAM and HAM documents, it was discovered that SPAM documents generally had higher frequencies of nouns and adjectives, and lower frequencies of verbs. This was gleaned at a corpus level, but this experiment will extract features at a document level to determine if they are useful for identifying SPAM.\n",
        "\n",
        "Baseline featureset:\n",
        "\n",
        "- Top 3000 unigrams and top 1000 bigrams based on frequency with basic stopwords applied, as demonstrated in Experiment 2\n",
        "Test featureset:\n",
        "\n",
        "- Top 3000 unigrams by frequency with basic stopwords exclusion\n",
        "- Top 1000 bigrams scored by frequency with basic stopwords exclusion\n",
        "- Normalized POS tag frequency (no stopwords removed) that aggregates for nouns, verbs, adjectives, and adverbs\n",
        "  - The default NLTK (Stanford) tagger will be used\n",
        "  - POS tags are known to be effective in certain circumstances such as with shorter sentence-level or social media posts (e.g. tweets)\n",
        "  \n",
        "Classifier: Naive Bayes Classifier from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ7w4rlApjW_"
      },
      "source": [
        "# [feature definition function: experiment 3] function to get document features (applicable for unigrams, bigrams, and pos tags)\n",
        "# param document: a list of strings representing a tokenized email\n",
        "# param word_features: a list of strings against which the tokens in document are matched\n",
        "# param bigram_features: a list of tuples where each element is a bigram\n",
        "# returns a dictionary where each key value is either 'contains(keyword)' and boolean or normalized frequencies by POS tags\n",
        "def pos_document_features(document, word_features, bigram_features):\n",
        "    document_words = set(document) # unigrams\n",
        "    document_bigrams = nltk.bigrams(document) # bigrams\n",
        "    document_pos = [t[1] for t in nltk.pos_tag(document)] # pos tags\n",
        "    features = {}\n",
        "    # unigram features\n",
        "    for word in word_features:\n",
        "        features['V_{}'.format(word)] = (word in document_words)\n",
        "    # bigram features\n",
        "    for bigram in bigram_features:\n",
        "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)\n",
        "    # pos features\n",
        "    noun_count, verb_count, adj_count, adv_count = 0, 0, 0, 0\n",
        "    for tag in document_pos:\n",
        "        if tag.startswith('N'): noun_count += 1\n",
        "        if tag.startswith('V'): verb_count += 1\n",
        "        if tag.startswith('J'): adj_count += 1\n",
        "        if tag.startswith('R'): adv_count += 1\n",
        "    features['noun_count_norm'] = noun_count / len(document_pos)\n",
        "    features['verb_count_norm'] = verb_count / len(document_pos)\n",
        "    features['adj_count_norm'] = adj_count / len(document_pos)\n",
        "    features['adv_count_norm'] = adv_count / len(document_pos)\n",
        "    return features    \n",
        "\n",
        "# function to get feature sets for modeling in Experiment 2\n",
        "def getFeatureSets3(documents, stopwords):\n",
        "    # get word features based on specified stopwords\n",
        "    word_features = getWordFeatures(documents, stopwords)\n",
        "    # get bigram features based on specified stopwords\n",
        "    bigram_features = getBigramFeatures(documents, stopwords)\n",
        "    featuresets = [(pos_document_features(d, word_features, bigram_features), c) for (d, c) in documents]\n",
        "    return featuresets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzQQCGJ2posk"
      },
      "source": [
        "Feature Extraction\n",
        "- Two feature sets that either use (unigrams + bigrams) or (unigrams + bigrams + normalized pos tag frequencies)\n",
        "  - bigramBasic4000 uses 3000 word features and 1000 bigram features having excluded basic stopwords (defined in Experiment 2)\n",
        "  - posBasic4000 uses 3000 word features, 1000 bigram features (as defined in Experiment 2), and 4 additional pos tag features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV8jI1ZOpySH"
      },
      "source": [
        "# get feature sets for documents using basic stopwords\n",
        "posBasic4000 = getFeatureSets3(documents, stopwords1)\n",
        "# split training and test sets 70/30\n",
        "posBasic4000Train = posBasic4000[:3620]\n",
        "posBasic4000Test = posBasic4000[3620:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSvyd_ujp0yT"
      },
      "source": [
        "## **Model Evaluation: Unigrams + Bigrams**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ximsXO2Xp22F"
      },
      "source": [
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, bigramBasic4000Train)\n",
        "# train a classifier and predict test set: get evaluation metrics; plot confusion matrix\n",
        "getClassScores(bigramBasic4000Train, bigramBasic4000Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX7Dkw5Kp-5q"
      },
      "source": [
        "**Model Evaluation: Unigrams + Bigrams + POS Tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUUkCoEtqAyo"
      },
      "source": [
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, posBasic4000Train)\n",
        "# train a classifier and predict test set: get evaluation metrics; plot confusion matrix\n",
        "getClassScores(posBasic4000Train, posBasic4000Test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbyVQvxGqJrt"
      },
      "source": [
        "# **Experiment 4: Additional Corpus Statistics as Features**\n",
        "The objective of this experiment is to test whether adding certain corpus statistics provides for improved classification results, where the \"corpus\" in this context is an email. In earlier exploration it was observed that SPAM emails were generally longer than HAM emails and contain more unique words. They also measured higher in lexical richness.\n",
        "\n",
        "Lexical richness, character count, and mean word length will be the features to include and test:\n",
        "\n",
        "1. Lexical richness is the ratio of unique word count to total word count\n",
        "Character count is the total number of characters in the email\n",
        "2. Mean word length is the average number of characters per word for every word in the email\n",
        "3. For the purposes of this experiment, stopwords will not be removed for the extraction of these three features.\n",
        "\n",
        "Baseline featureset:\n",
        "\n",
        "- 3000 unigrams, 1000 bigrams, and POS tags as defined in Experiment 3\n",
        "Test featureset:\n",
        "\n",
        "- Top 3000 unigrams by frequency with basic stopwords exclusion\n",
        "- Top 1000 bigrams scored by frequency with basic stopwords exclusion\n",
        "- Normalized POS tag frequency (no stopwords removed) that aggregates for nouns, verbs, adjectives, and adverbs using the default NLTK (Stanford) tagger\n",
        "- Lexical richness, email character count, and mean word length\n",
        "\n",
        "Classifier: Naive Bayes Classifier from NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIU8WN7bqbV5"
      },
      "source": [
        "\n",
        "# [feature definition function: experiment 4] function to get document features\n",
        "# param document: a list of strings representing a tokenized email\n",
        "# param word_features: a list of strings against which the tokens in document are matched\n",
        "# param bigram_features: a list of tuples where each element is a bigram\n",
        "# returns a dictionary where each key value is either 'contains(keyword)', normalized frequencies by POS tags, or corpus statistics\n",
        "def document_features4(document, word_features, bigram_features):\n",
        "    document_words = set(document) # unigrams\n",
        "    document_bigrams = nltk.bigrams(document) # bigrams\n",
        "    document_pos = [t[1] for t in nltk.pos_tag(document)] # pos tags\n",
        "    features = {}\n",
        "    # unigram features\n",
        "    for word in word_features:\n",
        "        features['V_{}'.format(word)] = (word in document_words)\n",
        "    # bigram features\n",
        "    for bigram in bigram_features:\n",
        "        features['B_{}_{}'.format(bigram[0], bigram[1])] = (bigram in document_bigrams)\n",
        "    # pos features\n",
        "    noun_count, verb_count, adj_count, adv_count = 0, 0, 0, 0\n",
        "    for tag in document_pos:\n",
        "        if tag.startswith('N'): noun_count += 1\n",
        "        if tag.startswith('V'): verb_count += 1\n",
        "        if tag.startswith('J'): adj_count += 1\n",
        "        if tag.startswith('R'): adv_count += 1\n",
        "    features['noun_count_norm'] = noun_count / len(document_pos)\n",
        "    features['verb_count_norm'] = verb_count / len(document_pos)\n",
        "    features['adj_count_norm'] = adj_count / len(document_pos)\n",
        "    features['adv_count_norm'] = adv_count / len(document_pos)\n",
        "    # corpus statistics\n",
        "    features['lexical_richness'] = len(document_words) / len(document) # lexical richness of email\n",
        "    features['total_char_count'] = 0 # total character count of email\n",
        "    for word in document:\n",
        "        features['total_char_count'] += len(word)\n",
        "    word_lengths = [len(word) for word in document]\n",
        "    features['mean_word_length'] = sum(word_lengths) / len(word_lengths) # mean word length of email\n",
        "    return features    \n",
        "\n",
        "# function to get feature sets for modeling in Experiment 2\n",
        "def getFeatureSets4(documents, stopwords):\n",
        "    # get word features based on specified stopwords\n",
        "    word_features = getWordFeatures(documents, stopwords)\n",
        "    # get bigram features based on specified stopwords\n",
        "    bigram_features = getBigramFeatures(documents, stopwords)\n",
        "    featuresets = [(document_features4(d, word_features, bigram_features), c) for (d, c) in documents]\n",
        "    return featuresets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnPnH9uAqmnB"
      },
      "source": [
        "**Feature Extraction**\n",
        "- Two feature sets that either use (unigrams + bigrams + normalized pos tag frequencies) or (unigrams + bigrams + normalized pos tag frequencies + corpus statistics)\n",
        "  - posBasic4000 uses 3000 word features, 1000 bigram features (as defined in Experiment 2), and 4 additional pos tag features (defined in Experiment 3)\n",
        "  - corpStats includes features defined in Experiment 3 as well as additional corpus statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ye4t-E7qxev"
      },
      "source": [
        "**Model Evaluation: Unigrams + Bigrams + POS Tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EhtmPFQq2j0"
      },
      "source": [
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, posBasic4000Train)\n",
        "# train a classifier and predict test set: get evaluation metrics; plot confusion matrix\n",
        "getClassScores(posBasic4000Train, posBasic4000Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRpXE-J2q4mR"
      },
      "source": [
        "**Model Evaluation: Unigrams + Bigrams + POS Tags + Corpus Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luv1OSKbq_Dj"
      },
      "source": [
        "# 5-fold cross-validation on training set\n",
        "cross_validation_accuracy(5, corpStatsTrain)\n",
        "# train a classifier and predict test set: get evaluation metrics; plot confusion matrix\n",
        "getClassScores(corpStatsTrain, corpStatsTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoSNkKkArEGe"
      },
      "source": [
        "# **Experiment 5: Comparison of Classification Algorithms in NLTK and Sci-Kit Learn**\n",
        "The objective of this experiment is to test the performance of a different classification algorithm and compare it to the best baseline model from Experiment 4. The featureset content will essentially remain unchanged, but the comparison will be on the different classifiers.\n",
        "\n",
        "Common featureset for both classifiers:\n",
        "\n",
        "- Top 3000 unigrams by frequency with basic stopwords exclusion\n",
        "- Top 1000 bigrams scored by frequency with basic stopwords exclusion\n",
        "- Normalized POS tag frequency (no stopwords removed) that aggregates for nouns, verbs, adjectives, and adverbs using the default NLTK (Stanford) tagger\n",
        "- Lexical richness, email character count, and mean word length\n",
        "\n",
        "The featureset may be formatted to an array/sparse matrix to comply with the Sci-Kit Learn classifier specifications. For the purposes of this experiment, the default modeling tuning parameters will be used.\n",
        "\n",
        "Baseline algorithm/classifier:\n",
        "\n",
        "- Naive Bayes Classifier from NLTK\n",
        "\n",
        "Test algorithm/classifier:\n",
        "\n",
        "- Linear SVC (support vector classification)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PXyT8tzrjl7"
      },
      "source": [
        "# **Feature Set Conversion for Sci-Kit Learn Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpGE0QldrnqH"
      },
      "source": [
        "#Using Pandas\n",
        "features = [f for (f,c) in corpStats]\n",
        "labels = [c for (f,c) in corpStats]\n",
        "# pandas data frame of features\n",
        "X = pd.DataFrame(features)\n",
        "y = np.array(labels)\n",
        "# train / test split (70/30)\n",
        "X_train = X.iloc[:3620, :]\n",
        "X_test = X.iloc[3620:, :]\n",
        "y_train = y[:3620]\n",
        "y_test = y[3620:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrmVoW1EsI2l"
      },
      "source": [
        "**Model Evaluation: Naive Bayes Classifier from NLTK**\n",
        "\n",
        "- 10-fold cross-validation accuracy, precision, recall, F1, and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Dlh_RfsRfr"
      },
      "source": [
        "# 10-fold cross-validation on training set\n",
        "cross_validation_accuracy(10, corpStatsTrain)\n",
        "# train a classifier and predict test set: get evaluation metrics; plot confusion matrix\n",
        "getClassScores(corpStatsTrain, corpStatsTest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-NBUreosd5B"
      },
      "source": [
        "**Model Evaluation: Linear SVC from Sci-Kit Learn**\n",
        "- 10-fold cross-validation accuracy, precision, recall, F1, and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u6VH1v9sjw0"
      },
      "source": [
        "# train classifier\n",
        "classifier = LinearSVC(C=1, penalty='l1', dual=False, class_weight='balanced')\n",
        "# 10-fold cross-validation on training set\n",
        "np.random.seed(111)\n",
        "y_pred = cross_val_predict(classifier, X_train, y_train, cv=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BP06SXisl6d"
      },
      "source": [
        "# classification report of cross validation results from training set\n",
        "print(classification_report(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iQ-hWiRsoDJ"
      },
      "source": [
        "# train classifer to predict test set\n",
        "svm = classifier.fit(X_train, y_train)\n",
        "preds = svm.predict(X_test)\n",
        "# evaluation measures\n",
        "eval_measures(y_test, preds)\n",
        "getCM(y_test, preds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}